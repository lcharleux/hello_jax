{"version":"1","records":[{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/intro-to-jax","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"","type":"content","url":"/intro-to-jax","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"La promesse de Jax"},"type":"lvl2","url":"/intro-to-jax#la-promesse-de-jax","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"La promesse de Jax"},"content":"\n\nJax promet de pouvoir écrire du code Python et de le déployer sur des plateformes CPU, GPU et TPU-Google sans efforts de traduction particuliers. Il permet aussi de fair des opérations (transformations) assez inédites comme la dérivation automatique des fonctions par rapport à leurs arguments ce qui constitue en soit un game changer pour l’IA et bien d’autres domaines.\n\n","type":"content","url":"/intro-to-jax#la-promesse-de-jax","position":3},{"hierarchy":{"lvl1":"Introduction","lvl2":"Le monde de Jax"},"type":"lvl2","url":"/intro-to-jax#le-monde-de-jax","position":4},{"hierarchy":{"lvl1":"Introduction","lvl2":"Le monde de Jax"},"content":"Avec des pincettes énormes, on pourrait résumer le monde de Jax à des données sous forme de tenseurs qui sont manipulées par des fonctions pures auxquelles on applique des transformations. Dans les nuances à apporter, il faut noter que la structure des données tensorielle est ou peut être agencée sous forme de pytrees ce qui une idées extrêmement puissante à elle seule, même si ce n’est pas ce qui saute aux yeux quand on début Jax.\n\n","type":"content","url":"/intro-to-jax#le-monde-de-jax","position":5},{"hierarchy":{"lvl1":"Introduction","lvl2":"Des fonctions pures ?"},"type":"lvl2","url":"/intro-to-jax#des-fonctions-pures","position":6},{"hierarchy":{"lvl1":"Introduction","lvl2":"Des fonctions pures ?"},"content":"Dans Jax, la pureté des fonctions est un sujet qui revient souvent. Une fonction pure est une fonction qui n’a pas d’effets de bords. Elle n’utilise donc pas l’infinité de bidouilles que Python autorise. Dans les grandes lignes, les sorties d’une fonction doivent dépendre de manière déterministe de ses arguments et uniquement d’eux. Cela interdit notamment l’usage de variables globales (enfin, on verra que c’est plus subtile) et aussi de modifier dynamiquement ses propres arguments comme on le ferait souvent en C.\nSi on pense C justement, on peut se dire cette approche est antinomique avec l’économie de mémoire et la performance en général, en fait oui et non. Jax impose cette contrainte car il va voir les fonctions comme des scripts à interpréter dans son langage (voir jaxpr) et à traduire dans un langage dédié à la plateforme cible. L’optimisation au sens ou la verrait en C n’a donc pas lieu d’être. Le but de la pureté est avant tout de lever toute ambiguïté sur le fonctionnement interne de la fonction et de pouvoir y tracer le chemin de l’information.\n\nVoici un petit exemple de fonction pure et de la manière sont jax la comprend:\n\nimport jax\nfrom jax import numpy as jnp\nimport time\n\n\ndef dumb_pure_func(x):\n    b = x + 3\n    c = b**2\n    return c\n\n\ndumb_pure_func(3)\n\njax.make_jaxpr(dumb_pure_func)(2)\n\nOn remarque Jax comprend bien le fonctionnement interne.\n\n","type":"content","url":"/intro-to-jax#des-fonctions-pures","position":7},{"hierarchy":{"lvl1":"Introduction","lvl2":"Les transformations"},"type":"lvl2","url":"/intro-to-jax#les-transformations","position":8},{"hierarchy":{"lvl1":"Introduction","lvl2":"Les transformations"},"content":"Imaginons qu’on travaille sur la fonction suivante:\n\ndef myfunc(x, a=1, b=1, c=1):\n    return a * x**2 + b * x + c\n\n\njax.make_jaxpr(myfunc)(3, 1, 1, 1)\n\nmyfunc(5)\n\n","type":"content","url":"/intro-to-jax#les-transformations","position":9},{"hierarchy":{"lvl1":"Introduction","lvl3":"Vectoriser avec vmap","lvl2":"Les transformations"},"type":"lvl3","url":"/intro-to-jax#vectoriser-avec-vmap","position":10},{"hierarchy":{"lvl1":"Introduction","lvl3":"Vectoriser avec vmap","lvl2":"Les transformations"},"content":"On peut vectoriser par rapport à un axe par exemple avec vmap.\n\nvmyfunc = jax.vmap(myfunc, in_axes=(0, None, None, None))\nxa = jnp.linspace(0.0, 5.0, 6)\nvmyfunc(xa, 1, 1, 1)\n\nMais on peut faire des structure bien plus complexes en combinant plusieurs transformations:\n\nvmyfunc2 = jax.vmap(vmyfunc, in_axes=(None, 0, None, None))\naa = jnp.linspace(0.0, 1.0, 3)\nvmyfunc2(xa, aa, 1, 1)\n\nLe potentiel est énorme car on peut soit vectoriser en plusieurs strates ou aussi le faire d’un coup en jouant sur les axes selon les besoins.\n\n","type":"content","url":"/intro-to-jax#vectoriser-avec-vmap","position":11},{"hierarchy":{"lvl1":"Introduction","lvl3":"Compiler avec jit","lvl2":"Les transformations"},"type":"lvl3","url":"/intro-to-jax#compiler-avec-jit","position":12},{"hierarchy":{"lvl1":"Introduction","lvl3":"Compiler avec jit","lvl2":"Les transformations"},"content":"Il est possible de compiler tout ou partie du code avec jit. La compilation va coûter quelques milisecondes et permettre une execution optimisée par la suite.\n\nNe = 100\nxa = jnp.linspace(0.0, 5.0, 6000)\naa = jnp.linspace(0.0, 1.0, 3000)\nt0 = time.time()\nfor e in range(Ne):\n    val = vmyfunc2(xa, aa, 1, 1)\n    val.block_until_ready()\nt1 = time.time()\ndt0 = (t1 - t0) / Ne\nprint(f\"Exectution took {dt0*1.e3:.2f} ms\")\n\njvmyfunc2 = jax.jit(vmyfunc2)\nt0 = time.time()\nval = jvmyfunc2(xa, aa, 1, 1)\nval.block_until_ready()\nt1 = time.time()\ndt1 = t1 - t0\nprint(f\"Compilation + first execution took {dt1*1.e3:.2f} ms\")\n\nt0 = time.time()\nfor e in range(Ne):\n    val = jvmyfunc2(xa, aa, 1, 1)\n    val.block_until_ready()\nt1 = time.time()\ndt2 = (t1 - t0) / Ne\nprint(f\"Second execution took {dt2*1.e3:.2f} ms\")\n\nOn a donc gagné du temps avec le jit et ce malgré le fait que notre fonction est très simple et donc très optimisée à la base. Cette tendance sera amplifiée sur des calculs lourds sur GPU/TPU.\n\n","type":"content","url":"/intro-to-jax#compiler-avec-jit","position":13},{"hierarchy":{"lvl1":"Introduction","lvl3":"Autres transformations","lvl2":"Les transformations"},"type":"lvl3","url":"/intro-to-jax#autres-transformations","position":14},{"hierarchy":{"lvl1":"Introduction","lvl3":"Autres transformations","lvl2":"Les transformations"},"content":"Les autres transformations ne sont pas cruciales maintenant alors je les passe sous couvert. Mais elles sont ultra intéressantes dans d’autres cas, surtout grad.\n\n\n\n","type":"content","url":"/intro-to-jax#autres-transformations","position":15},{"hierarchy":{"lvl1":"Introduction","lvl2":"Liste non exhaustive des limitations de Jax"},"type":"lvl2","url":"/intro-to-jax#liste-non-exhaustive-des-limitations-de-jax","position":16},{"hierarchy":{"lvl1":"Introduction","lvl2":"Liste non exhaustive des limitations de Jax"},"content":"Forcément, cette belle promesse vient avec pas mal de limitations.","type":"content","url":"/intro-to-jax#liste-non-exhaustive-des-limitations-de-jax","position":17},{"hierarchy":{"lvl1":"Introduction","lvl3":"Les structures de contrôle","lvl2":"Liste non exhaustive des limitations de Jax"},"type":"lvl3","url":"/intro-to-jax#les-structures-de-contr-le","position":18},{"hierarchy":{"lvl1":"Introduction","lvl3":"Les structures de contrôle","lvl2":"Liste non exhaustive des limitations de Jax"},"content":"On commence par une des plus agacentes au début, les structures de contrôle. Fini les if, foret while.\n\nEn fait, ces dernières ne sont pas claires dans leurs buts et peuvent correspondre à plusieurs objectifs. Jax fournit donc des outils de remplacements qui ne manqueront pas de vous énerver (parfois). A titre d’exemple, forsera remplacée alternativement selon les buts par vmap, scan, where, lax.fori_loop ou pourra rester for dans ces bien choisis.\n\n","type":"content","url":"/intro-to-jax#les-structures-de-contr-le","position":19},{"hierarchy":{"lvl1":"Introduction","lvl3":"L’allocation dynamique de mémoire","lvl2":"Liste non exhaustive des limitations de Jax"},"type":"lvl3","url":"/intro-to-jax#lallocation-dynamique-de-m-moire","position":20},{"hierarchy":{"lvl1":"Introduction","lvl3":"L’allocation dynamique de mémoire","lvl2":"Liste non exhaustive des limitations de Jax"},"content":"Dans le monde de jax, il est interdit d’allouer dynamiquement de la mémoire, par exemple en créant des array de taille inconnue à la compilation. Cela ne manquera pas de vous créer des frustrations. On verra aussi qu’il est possible de trouver des compromis sur ce point. Le chapitre des sharp bits et globalement toutes les prises de parole de JakeVPD et Patrick Kidger mérite d’être lues pour comprendre la parole sainte à ce sujet.\n\nExemple:\n\ndef dumb_func_allocating_memory(n):\n    a = jnp.arange(n)\n    return a\n\n# jax.make_jaxpr(dumb_func_allocating_memory)(2) # Uncommment to see the error.\n\n\n\nConcretizationTypeError                   Traceback (most recent call last)\nCell In[11], line 1\n----> 1 jax.make_jaxpr(dumb_func_allocating_memory)(2)[... skipping hidden 14 frame]\n\nCell In[10], line 2, in dumb_func_allocating_memory(n)\n1 def dumb_func_allocating_memory(n):\n----> 2     a = jnp.arange(n)\n3     return a\n\nFile ~/miniforge3/envs/science/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:5947, in arange(start, stop, step, dtype, device, out_sharding)\n5945 if sharding is None or not sharding._is_concrete:\n5946   assert sharding is None or isinstance(sharding, NamedSharding)\n-> 5947   return _arange(start, stop=stop, step=step, dtype=dtype,\n5948                  out_sharding=sharding)\n5949 else:\n5950   output = _arange(start, stop=stop, step=step, dtype=dtype)\n\nFile ~/miniforge3/envs/science/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:5962, in _arange(start, stop, step, dtype, out_sharding)\n5960 util.check_arraylike(“arange”, start)\n5961 if stop is None and step is None:\n-> 5962   start = core.concrete_or_error(None, start, “It arose in the jnp.arange argument ‘stop’”)\n5963 else:\n5964   start = core.concrete_or_error(None, start, “It arose in the jnp.arange argument ‘start’”)\n\nFile ~/miniforge3/envs/science/lib/python3.12/site-packages/jax/_src/core.py:1847, in concrete_or_error(force, val, context)\n1845 maybe_concrete = val.to_concrete_value()\n1846 if maybe_concrete is None:\n-> 1847   raise ConcretizationTypeError(val, context)\n1848 else:\n1849   return force(maybe_concrete)\n\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape int32[]\nIt arose in the jnp.arange argument ‘stop’\nThe error occurred while tracing the function dumb_func_allocating_memory at /var/folders/67/hblp6z8n36ldk_9_bl9g80kh0000gn/T/ipykernel_50677/3347747001.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument n.\n\nSee \n\nhttps://​docs​.jax​.dev​/en​/latest​/errors​.html​#jax​.errors​.ConcretizationTypeError\n\nFrustration, colère ...\n\nDans un tel, cas il faut généralement se demander si on a vraiment besoin que nsoit dynamique. Si c’est vraiment le cas, alors on peut le rendre statique (au sens jax) en spécifiant:\n\nimport numpy as np\n\n\ndef make_dumb_function_allocating_memory(n):\n    def dumb_func_allocating_memory2(a):\n        x = a * jnp.arange(n)\n        return x\n\n    return dumb_func_allocating_memory2\n\n\ndfam = jax.jit(make_dumb_function_allocating_memory(3))\n\njax.make_jaxpr(dfam)(2)","type":"content","url":"/intro-to-jax#lallocation-dynamique-de-m-moire","position":21},{"hierarchy":{"lvl1":"Des ODE avec Jax ?"},"type":"lvl1","url":"/ode-with-jax","position":0},{"hierarchy":{"lvl1":"Des ODE avec Jax ?"},"content":"%matplotlib ipympl\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom jax import jit, vmap\nfrom dataclasses import dataclass\n\n","type":"content","url":"/ode-with-jax","position":1},{"hierarchy":{"lvl1":"Des ODE avec Jax ?","lvl2":"Problème"},"type":"lvl2","url":"/ode-with-jax#probl-me","position":2},{"hierarchy":{"lvl1":"Des ODE avec Jax ?","lvl2":"Problème"},"content":"On veut intégrer l’ODE suivante:\\left \\lbrace \n\\begin{split}\nM_1  \\ddot x_1 + D_1  \\dot x_1 + K_1  x_1 + K_{12} (x_1 - x_2) = F_{d1} \\sin (w_d t)  \\\\\nM_2  \\ddot x_2 + D_2  \\dot x_2 + K_2  x_1 + K_{12} (x_2 - x_1) = F_{d2} \\sin (w_d t) \n\\end{split}\n\\right .\n\nNote\n\nIl est possible de simplifier grandement cette équation en l’adimensionnant mais on fait le choix de ne pas le faire ici.\n\nOn la traduit comme suit en Python:\n\nOn crée une classe pour stocker les paramètres du problème:\n\n@dataclass\nclass CoupleLinearResonatorParams:\n    \"\"\"\n    Parameters for the coupled linear resonator ODE system.\n    \"\"\"\n\n    M1: float = 1.0  # Mass 1\n    M2: float = 1.0  # Mass 2\n    K1: float = 1.0  # Spring constant 1\n    K2: float = 1.0  # Spring constant 2\n    K12: float = 0.5  # Coupling spring constant 2\n    D1: float = 0.2  # Damping coefficient 1\n    D2: float = 0.2  # Damping coefficient 2\n    wd: float = 0.5  # Driving frequency\n    Fd1: float = 0.5  # Driving force amplitude 1\n    Fd2: float = 0.5  # Driving force amplitude 2\n\n\node_params = CoupleLinearResonatorParams()\node_params\n\ndef coupled_linear_resonator_ode(t, X, params: CoupleLinearResonatorParams):\n    x1, v1, x2, v2 = X\n    dx1dt = v1\n    dv1dt = (\n        -params.K1 * x1\n        - params.K12 * (x1 - x2)\n        - params.D1 * v1\n        + params.Fd1 * jnp.sin(params.wd * t)\n    ) / params.M1\n    dx2dt = v2\n    dv2dt = (\n        -params.K2 * x2\n        - params.K12 * (x2 - x1)\n        - params.D2 * v2\n        + params.Fd2 * jnp.sin(params.wd * t)\n    ) / params.M2\n    return jnp.array([dx1dt, dv1dt, dx2dt, dv2dt])","type":"content","url":"/ode-with-jax#probl-me","position":3}]}